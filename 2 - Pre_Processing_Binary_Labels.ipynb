{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import impute, preprocessing, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from numpy import loadtxt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_DDos = pd.read_csv('CICIDS2017_Datasets\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', sep=',')\n",
    "dados_PortScan = pd.read_csv('CICIDS2017_Datasets\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', sep=',')\n",
    "dados_Bot = pd.read_csv('CICIDS2017_Datasets\\Friday-WorkingHours-Morning.pcap_ISCX.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete rows with NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [dados_DDos,dados_PortScan,dados_Bot]:\n",
    "    i.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dados_DDos,dados_PortScan,dados_Bot:\n",
    "\n",
    "    df.drop(['Flow ID'],axis=1,inplace=True)\n",
    "    df.drop([' Timestamp'],axis=1,inplace=True)\n",
    "    df.drop([' Source IP'], axis=1,inplace=True)\n",
    "    df.drop([' Destination IP'], axis=1,inplace=True)\n",
    "    df[' Bwd Packet Length Std']=df[' Bwd Packet Length Std'].astype(np.float)\n",
    "    df['Flow Bytes/s']=df['Flow Bytes/s'].astype(np.float)\n",
    "    df[' Flow Packets/s']=df[' Flow Packets/s'].astype(np.float)\n",
    "    df[' Flow IAT Std']=df[' Flow IAT Std'].astype(np.float)\n",
    "    df['Fwd Packets/s']=df['Fwd Packets/s'].astype(np.float)\n",
    "    df[' Bwd Packets/s']=df[' Bwd Packets/s'].astype(np.float)\n",
    "    df[' Packet Length Mean']=df[' Packet Length Mean'].astype(np.float)\n",
    "    df[' Packet Length Std']=df[' Packet Length Std'].astype(np.float)\n",
    "    df[' Packet Length Variance']=df[' Packet Length Variance'].astype(np.float)\n",
    "    df[' Average Packet Size']=df[' Average Packet Size'].astype(np.float)\n",
    "    df[' Avg Fwd Segment Size']=df[' Avg Fwd Segment Size'].astype(np.float)\n",
    "    df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Column Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name_DDos = dados_DDos[' Label'].unique()\n",
    "class_name_PortScan = dados_PortScan[' Label'].unique()\n",
    "class_name_Bot = dados_Bot[' Label'].unique()\n",
    "\n",
    "for i in [dados_DDos,dados_PortScan,dados_Bot]:\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    input_classes = i[' Label'].unique()\n",
    "    label_encoder.fit(input_classes)\n",
    "    i[' Label'] = label_encoder.transform(i[' Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative values check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [dados_DDos,dados_PortScan,dados_Bot]:\n",
    "    columns = df.columns\n",
    "    #print('----------------------------')\n",
    "    for i in columns:\n",
    "        if (df[i] < 0).any():\n",
    "            number_of_negatives = np.sum((df[i] < 0).values.ravel())\n",
    "           # print(i + ' : '+ str(number_of_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop single unique value features and the two features with high number of negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dados_DDos,dados_PortScan,dados_Bot:\n",
    "    i.drop([' Bwd PSH Flags'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd URG Flags'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd URG Flags'], axis=1,inplace=True)\n",
    "    i.drop([' CWE Flag Count'], axis=1,inplace=True)\n",
    "    i.drop(['Fwd Avg Bytes/Bulk'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Avg Packets/Bulk'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Avg Bulk Rate'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Avg Bytes/Bulk'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Avg Packets/Bulk'], axis=1,inplace=True)\n",
    "    i.drop(['Bwd Avg Bulk Rate'], axis=1,inplace=True)\n",
    "    i.drop(['Init_Win_bytes_forward'], axis=1,inplace=True) #muitos valores negativos\n",
    "    i.drop([' Init_Win_bytes_backward'], axis=1,inplace=True) #muitos valores negativos\n",
    "\n",
    "dados_DDos = dados_DDos.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_PortScan = dados_PortScan.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_Bot = dados_Bot.replace([np.inf, -np.inf], np.nan).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all rows with negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Convert negative values to NaN \n",
    "dados_DDos_without_neg = dados_DDos[dados_DDos >= 0]\n",
    "dados_PortScan_without_neg = dados_PortScan[dados_PortScan >= 0]\n",
    "dados_Bot_without_neg = dados_Bot[dados_Bot >= 0]\n",
    "\n",
    "############# Remove the rows with NaN \n",
    "dados_DDos_without_neg = dados_DDos_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_PortScan_without_neg = dados_PortScan_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_Bot_without_neg = dados_Bot_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the variables correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dados_DDos_without_neg.corr().abs()\n",
    "s = c.unstack()\n",
    "print(s[' Label'])\n",
    "so = s.sort_values(kind=\"quicksort\")\n",
    "print(so[-61:-1])\n",
    "\n",
    "filtered_correlation = {}\n",
    "for item in so.iteritems(): # retorna os valores das correlações entre 0.7 e 1 sem repetições\n",
    "    a = item[0][0]\n",
    "    b = item[0][1]\n",
    "    if (item[1] >= 0.7 and  item[1] < 1):\n",
    "        if not(b in filtered_correlation and a in filtered_correlation[b]):\n",
    "            if a not in filtered_correlation:\n",
    "                filtered_correlation[a] = {}\n",
    "            filtered_correlation[a][b]=item[1]\n",
    "\n",
    "for i in filtered_correlation: # imprime os valores de forma bonitinha\n",
    "    for j in filtered_correlation[i]:\n",
    "        print(i + '\\t' + j + '\\t' + str(filtered_correlation[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the variables more than 70% correlated, remove one of variables of each correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################--- Remover as variáveis + de 70% correlacionadas ----##################################\n",
    "for i in dados_DDos_without_neg,dados_PortScan_without_neg,dados_Bot_without_neg:\n",
    "    i.drop([' Total Backward Packets'], axis=1,inplace=True)\n",
    "    i.drop([' Total Length of Bwd Packets'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Packet Length Std'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Packet Length Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Flow IAT Std'], axis=1,inplace=True)\n",
    "    i.drop(['Fwd IAT Total'], axis=1,inplace=True)\n",
    "    i.drop([' Packet Length Std'], axis=1,inplace=True)\n",
    "    i.drop([' Packet Length Variance'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Header Length.1'], axis=1,inplace=True)\n",
    "    i.drop(['Subflow Fwd Packets'], axis=1,inplace=True)\n",
    "    i.drop([' Subflow Bwd Packets'], axis=1,inplace=True)\n",
    "    i.drop([' Subflow Bwd Bytes'], axis=1,inplace=True)\n",
    "    i.drop([' Active Max'], axis=1,inplace=True)\n",
    "    i.drop([' Active Min'], axis=1,inplace=True)\n",
    "    i.drop(['Idle Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Idle Min'], axis=1,inplace=True)\n",
    "    i.drop([' Idle Max'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd IAT Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd IAT Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Destination Port'], axis=1,inplace=True)\n",
    "    i.drop(['Bwd Packet Length Max'], axis=1,inplace=True)\n",
    "    i.drop([' Avg Fwd Segment Size'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Packet Length Min'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Header Length'], axis=1,inplace=True)\n",
    "    i.drop([' Max Packet Length'], axis=1,inplace=True)\n",
    "    i.drop([' PSH Flag Count'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd IAT Std'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd IAT Max'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Packet Length Max'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd IAT Std'], axis=1,inplace=True)\n",
    "    i.drop([' Min Packet Length'], axis=1,inplace=True)\n",
    "    i.drop([' Flow Duration'], axis=1,inplace=True)\n",
    "    i.drop([' Flow IAT Min'], axis=1,inplace=True) \n",
    "    i.drop([' Flow IAT Max'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd IAT Min'], axis=1,inplace=True)\n",
    "    i.drop(['Fwd Packets/s'], axis=1,inplace=True)\n",
    "    i.drop([' ECE Flag Count'], axis=1,inplace=True)\n",
    "    i.drop([' Subflow Fwd Bytes'], axis=1,inplace=True)\n",
    "    i.drop([' SYN Flag Count'], axis=1,inplace=True)\n",
    "    i.drop([' Average Packet Size'], axis=1,inplace=True)\n",
    "    i.drop([' Packet Length Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Bwd Packet Length Mean'], axis=1,inplace=True)\n",
    "    i.drop([' Fwd Packet Length Std'], axis=1,inplace=True)\n",
    "\n",
    "dados_DDos_without_neg.drop([' act_data_pkt_fwd'] , axis=1,inplace=True)\n",
    "dados_DDos_without_neg.drop([' Fwd IAT Max'] , axis=1,inplace=True)\n",
    "dados_DDos_without_neg.drop([' Total Fwd Packets'] , axis=1,inplace=True)\n",
    "dados_PortScan_without_neg.drop([' act_data_pkt_fwd'] , axis=1,inplace=True)\n",
    "dados_PortScan_without_neg.drop([' Flow IAT Mean'] , axis=1,inplace=True)\n",
    "dados_PortScan_without_neg.drop([' Fwd Header Length'] , axis=1,inplace=True)\n",
    "dados_PortScan_without_neg.drop([' Fwd Packet Length Min'] , axis=1,inplace=True)\n",
    "dados_Bot_without_neg.drop([' Flow IAT Mean'] , axis =1,inplace=True)\n",
    "dados_Bot_without_neg.drop([' Total Fwd Packets'] , axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [dados_DDos_without_neg,dados_PortScan_without_neg,dados_Bot_without_neg]:\n",
    "    columns = i.columns\n",
    "    for j in columns:\n",
    "        max=i[j].max()\n",
    "        if(max>10000):\n",
    "            #print(j)\n",
    "            #print(max)\n",
    "            i[j]=np.log(1 + i[j])\n",
    "\n",
    "dados_DDos_without_neg = dados_DDos_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_PortScan_without_neg = dados_PortScan_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "dados_Bot_without_neg = dados_Bot_without_neg.replace([np.inf, -np.inf], np.nan).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0,1]\n",
    "features_normalized_DDoS = preprocessing.normalize(dados_DDos_without_neg,norm='max',axis=0)\n",
    "features_normalized_DDoS = pd.DataFrame(features_normalized_DDoS, columns=dados_DDos_without_neg.columns)\n",
    "#features_normalized_DDoS['Total Length of Fwd Packets'].hist()\n",
    "#plt.savefig('After_Normalisation_Fwd_Header_Length.png')\n",
    "\n",
    "features_normalized_PortScan = preprocessing.normalize(dados_PortScan_without_neg,norm='max',axis=0)\n",
    "features_normalized_PortScan = pd.DataFrame(features_normalized_PortScan, columns=dados_PortScan_without_neg.columns)\n",
    "\n",
    "features_normalized_bot = preprocessing.normalize(dados_Bot_without_neg,norm='max',axis=0)\n",
    "features_normalized_bot = pd.DataFrame(features_normalized_bot, columns=dados_Bot_without_neg.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Rows in original data\n",
    "duplicateRowsDF = features_normalized_DDoS[features_normalized_DDoS.duplicated()]\n",
    "#print(duplicateRowsDF.count())\n",
    "features_normalized_DDoS=features_normalized_DDoS.drop_duplicates()\n",
    "\n",
    "duplicateRowsDF = features_normalized_PortScan[features_normalized_PortScan.duplicated()]\n",
    "#print(duplicateRowsDF.count())\n",
    "features_normalized_PortScan=features_normalized_PortScan.drop_duplicates()\n",
    "\n",
    "duplicateRowsDF = features_normalized_bot[features_normalized_bot.duplicated()]\n",
    "#print(duplicateRowsDF.count())\n",
    "features_normalized_bot=features_normalized_bot.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_normalized_DDoS = shuffle(features_normalized_DDoS)\n",
    "features_normalized_PortScan = shuffle(features_normalized_PortScan)\n",
    "features_normalized_bot = shuffle(features_normalized_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDoS_benign = features_normalized_DDoS[' Label'] == 0\n",
    "DDoS_attack = features_normalized_DDoS[' Label'] == 1\n",
    "\n",
    "PortScan_benign = features_normalized_PortScan[' Label'] == 0\n",
    "PortScan_attack = features_normalized_PortScan[' Label'] == 1\n",
    "\n",
    "Botnet_benign = features_normalized_bot[' Label'] == 0\n",
    "Botnet_attack = features_normalized_bot[' Label'] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 66-33 Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDoS_Benign_train1, DDoS_Benign_train2, DDoS_Benign_test = np.array_split(features_normalized_DDoS[DDoS_benign], 3)\n",
    "DDoS_Attack_train1, DDoS_Attack_train2, DDoS_attack_test = np.array_split(features_normalized_DDoS[DDoS_attack], 3)\n",
    "\n",
    "PortScan_Benign_train1, PortScan_Benign_train2, PortScan_Benign_test = np.array_split(features_normalized_PortScan[PortScan_benign], 3)\n",
    "PortScan_Attack_train1, PortScan_Attack_train2, PortScan_Attack_test = np.array_split(features_normalized_PortScan[PortScan_attack], 3)\n",
    "\n",
    "Bot_Benign_train1, Bot_Benign_train2, Bot_Bening_test = np.array_split(features_normalized_bot[Botnet_benign], 3)\n",
    "Bot_Attack_train1, Bot_Attack_train2, Bot_Attack_test = np.array_split(features_normalized_bot[Botnet_attack], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all train and test data and shuffle\n",
    "\n",
    "DDoS_train = pd.concat([DDoS_Benign_train1, DDoS_Benign_train2, DDoS_Attack_train1, DDoS_Attack_train2])\n",
    "DDoS_test = pd.concat([DDoS_Benign_test, DDoS_attack_test])\n",
    "DDoS_train = shuffle(DDoS_train)\n",
    "DDoS_test = shuffle(DDoS_test)\n",
    "\n",
    "PortScan_train = pd.concat([PortScan_Benign_train1, PortScan_Benign_train2, PortScan_Attack_train1, PortScan_Attack_train2])\n",
    "PortScan_test = pd.concat([PortScan_Benign_test, PortScan_Attack_test])\n",
    "PortScan_train = shuffle(PortScan_train)\n",
    "PortScan_test = shuffle(PortScan_test)\n",
    "\n",
    "Bot_train = pd.concat([Bot_Benign_train1, Bot_Benign_train2, Bot_Attack_train1, Bot_Attack_train2,])\n",
    "Bot_test = pd.concat([Bot_Bening_test, Bot_Attack_test])\n",
    "Bot_train = shuffle(Bot_train)\n",
    "Bot_test = shuffle(Bot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train and test data in csv files\n",
    "\n",
    "DDoS_train.to_csv(r'Train_test_data/DDoS_train.csv', index = False)\n",
    "DDoS_test.to_csv(r'Train_test_data/DDoS_test.csv', index = False)\n",
    "\n",
    "PortScan_train.to_csv(r'Train_test_data/PortScan_train.csv', index = False)\n",
    "PortScan_test.to_csv(r'Train_test_data/PortScan_test.csv', index = False)\n",
    "\n",
    "Bot_train.to_csv(r'Train_test_data/Bot_train.csv', index = False)\n",
    "Bot_test.to_csv(r'Train_test_data/Bot_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for the GAN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_Bot =  pd.concat([Bot_Attack_train1, Bot_Attack_train2,])\n",
    "GAN_Bot = shuffle(GAN_Bot)\n",
    "GAN_Bot.to_csv(r'GAN_generation\\Bot_Attack_Data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the amount of data needed for GAN generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(Bot_train[' Label'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
